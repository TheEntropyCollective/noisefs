# NoiseFS Scale Testing Strategy

This guide provides comprehensive solutions for testing NoiseFS at different scales, from 1 node to 10,000 nodes.

## ðŸŽ¯ Quick Decision Guide

| Scale | Nodes | Method | Command |
|-------|-------|--------|---------|
| Unit Testing | 1 | Single Node | `go run cmd/benchmarks/benchmark/main.go` |
| Small Team | 2-5 | Real IPFS | `go run cmd/benchmarks/benchmark/main.go -nodes 3` |
| Department | 10-50 | Simulation | `go run cmd/simulation/main.go -scenario small` |
| Organization | 100-500 | Simulation | `go run cmd/simulation/main.go -scenario medium` |
| Enterprise | 1000+ | Simulation | `go run cmd/simulation/main.go -scenario large` |
| Internet Scale | 10000+ | Simulation | `go run cmd/simulation/main.go -scenario massive` |

## ðŸ“Š Testing Methods Comparison

### 1. **Single Node Testing** (Most Accurate for Unit Performance)
```bash
go run cmd/benchmarks/benchmark/main.go -files 50 -verbose
```
- âœ… Real IPFS operations
- âœ… Accurate latency measurements
- âœ… Perfect for optimization work
- âŒ No network effects

### 2. **Multi-Node Real IPFS** (2-5 nodes)
```bash
# Improved version with offline mode
go run cmd/benchmarks/benchmark/main.go -nodes 3 -files 20 -verbose
```
- âœ… Real network behavior
- âœ… Cross-node replication
- âš ï¸  Limited to ~5 nodes due to resource constraints
- âŒ Complex setup

### 3. **Network Simulation** (10-10,000 nodes)
```bash
# Run all scenarios
go run cmd/simulation/main.go -scenario all -duration 60s

# Custom configuration
go run cmd/simulation/main.go -nodes 500 -files 5000 -duration 120s
```
- âœ… Massive scale testing
- âœ… Realistic content patterns
- âœ… No resource limitations
- âœ… Reproducible results
- âš ï¸  Simulated (not real IPFS)

### 4. **Docker Cluster** (Production-like)
```bash
docker-compose -f docker-compose.test.yml up -d
go run cmd/benchmarks/docker-benchmark/main.go -nodes 5 -verbose
```
- âœ… Production-like environment
- âœ… Container isolation
- âœ… Real IPFS nodes
- âŒ Resource intensive

## ðŸš€ Recommended Testing Pipeline

### Phase 1: Development (Daily)
```bash
# Quick performance check
go run cmd/benchmarks/benchmark/main.go -files 10 -verbose
```

### Phase 2: Integration (Pre-commit)
```bash
# Test with 3 nodes
go run cmd/benchmarks/benchmark/main.go -nodes 3 -files 20

# Simulate 100 nodes
go run cmd/simulation/main.go -scenario medium -duration 30s
```

### Phase 3: Release Testing
```bash
# Full simulation suite
go run cmd/simulation/main.go -scenario all -duration 300s

# Docker cluster test
docker-compose -f docker-compose.test.yml up -d
go run cmd/benchmarks/docker-benchmark/main.go -nodes 5 -files 50
```

### Phase 4: Scale Validation
```bash
# Test at different scales
for scenario in small medium large massive; do
    go run cmd/simulation/main.go -scenario $scenario -duration 60s
done
```

## ðŸ“ˆ Key Metrics to Track

### Performance Metrics
- **Latency**: Upload/download response times
- **Throughput**: MB/s transfer rates
- **Success Rate**: Operation completion percentage
- **Cache Hit Rate**: Efficiency of caching layer

### Scalability Metrics
- **Block Reuse Rate**: Content deduplication efficiency
- **Storage Overhead**: Actual vs theoretical storage
- **Network Efficiency**: Cross-node communication costs
- **Memory per Node**: Resource requirements

### NoiseFS-Specific Metrics
- **Anonymization Overhead**: Cost of privacy features
- **Randomizer Efficiency**: Smart selection effectiveness
- **Descriptor Distribution**: Metadata propagation speed

## ðŸ”§ Advanced Testing

### Custom Network Topologies
```go
// Create custom scenarios in cmd/simulation/main.go
runner.AddScenario(&simulation.Scenario{
    Name:        "Geographic Distribution",
    NodeCount:   200,
    FileCount:   1000,
    CacheSize:   500,
    Duration:    5 * time.Minute,
    Description: "Simulating globally distributed nodes",
})
```

### Performance Profiling
```bash
# CPU profiling
go run -cpuprofile=cpu.prof cmd/simulation/main.go -scenario large

# Memory profiling  
go run -memprofile=mem.prof cmd/simulation/main.go -scenario large

# Analyze profiles
go tool pprof cpu.prof
```

### Comparative Analysis
```bash
# Compare different cache sizes
for cache in 100 500 1000; do
    go run cmd/simulation/main.go -nodes 100 -cache $cache -output results-cache-$cache.json
done
```

## ðŸŽ¯ Best Practices

1. **Start Small**: Begin with single-node tests for baseline performance
2. **Scale Gradually**: Test at 10x increments (1, 10, 100, 1000 nodes)
3. **Use Simulation for Scale**: Real nodes for accuracy, simulation for scale
4. **Monitor Trends**: Track metrics over time, not just absolute values
5. **Reproducible Tests**: Use fixed seeds and configurations for consistency

## ðŸ“Š Example Results Interpretation

### Good Performance Indicators
- Block reuse rate > 30% (indicates effective deduplication)
- Storage overhead < 2x (efficient compared to 9-29x for other anonymous systems)
- Cache hit rate > 50% (effective caching strategy)
- Linear scaling of throughput with nodes

### Warning Signs
- Storage overhead > 3x (poor block reuse)
- Cache hit rate < 20% (ineffective caching)
- Throughput degradation with scale (bottlenecks)
- High memory per node (>100MB for small networks)

## ðŸš€ Quick Start Examples

```bash
# Test your changes quickly
go run cmd/benchmarks/benchmark/main.go -files 10

# Validate at moderate scale
go run cmd/simulation/main.go -nodes 100 -duration 30s

# Full scale validation
go run cmd/simulation/main.go -scenario all -comparison

# Production readiness
docker-compose -f docker-compose.test.yml up -d
go run cmd/benchmarks/docker-benchmark/main.go -nodes 5 -files 100
```